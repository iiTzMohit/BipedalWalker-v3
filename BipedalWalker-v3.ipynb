{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7cf73-584d-47ff-9b54-290405020973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57731a7-d01c-4f27-bf87-55e2ab73a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super().__init__()\n",
    "        self.mean = nn.Sequential(\n",
    "                        nn.Linear(obs_size, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(256, 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64, action_size),\n",
    "                        nn.Tanh())\n",
    "        \n",
    "        self.logstd_layer = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "        # Initialize weights with random normal distribution (stddev=0.01)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.mean:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mean(x), self.logstd_layer\n",
    "\n",
    "class ValueModel(nn.Module):\n",
    "    def __init__(self, obs_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                        nn.Linear(obs_size, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(256, 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64,1))\n",
    "\n",
    "        # Initialize weights with random normal distribution (stddev=0.01)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f0608-2839-472d-be5c-4950bace835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, dones, gamma=0.99):\n",
    "    discounted_rewards = [rewards[-1]]\n",
    "    for i in range(len(rewards) - 2, -1, -1):\n",
    "        if dones[i]:  # If the current state is terminal, reset the discounting\n",
    "            discounted_rewards.append(rewards[i])\n",
    "        else:\n",
    "            discounted_rewards.append(rewards[i] + gamma * discounted_rewards[-1])\n",
    "    discounted_r = np.array(discounted_rewards[::-1])\n",
    "    #discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "    #discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "    return discounted_r\n",
    "    \n",
    "def compute_gaes(rewards, values, dones, gamma=0.99, decay=0.90, normalize=True):\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val * (1 - done) - val for rew, val, next_val, done in zip(rewards, values, next_values, dones)]\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in range(len(deltas) - 2, -1, -1):\n",
    "        if dones[i]:  # If the next state is terminal, reset the GAE\n",
    "            gaes.append(deltas[i])\n",
    "        else:\n",
    "            gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "    gaes = np.array(gaes[::-1])\n",
    "    if normalize:\n",
    "        gaes = (gaes - np.mean(gaes)) / (np.std(gaes) + 1e-8)\n",
    "    return gaes\n",
    "\n",
    "def log_prob(action, mean, logstd):\n",
    "    std = np.exp(logstd)\n",
    "    std = np.clip(std, a_min=1e-8, a_max=None)  \n",
    "    log_prob = -0.5 * (((action - mean) / std) ** 2 + 2 * logstd + np.log(2 * np.pi))\n",
    "    return log_prob\n",
    "\n",
    "def tensor_log_prob(action, mean, logstd):\n",
    "    std = torch.exp(logstd)\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    return dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ec3ca-2976-4a0e-b705-acb125f9a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "  def __init__(self,\n",
    "              actor,\n",
    "              critic,\n",
    "              ppo_clip_val=0.2,\n",
    "              entropy_coeff = 0.01,\n",
    "              policy_train_iters=20,\n",
    "              value_train_iters=20,\n",
    "              policy_lr=3e-4,\n",
    "              value_lr=1e-2,\n",
    "              batch_size=64):\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.ppo_clip_val = ppo_clip_val\n",
    "    self.entropy_coeff = entropy_coeff\n",
    "    self.policy_train_iters = policy_train_iters\n",
    "    self.value_train_iters = value_train_iters\n",
    "    self.policy_optim = optim.Adam(self.actor.parameters(), lr=policy_lr)\n",
    "    self.value_optim = optim.Adam(self.critic.parameters(), lr=value_lr)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        dataset_size = obs.size(0)\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        for _ in range(self.policy_train_iters):\n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "    \n",
    "                batch_obs = obs[batch_indices]\n",
    "                batch_acts = acts[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_gaes = gaes[batch_indices]\n",
    "                \n",
    "                self.policy_optim.zero_grad()\n",
    "                mean, logstd = self.actor(batch_obs)\n",
    "                new_log_probs = tensor_log_prob(batch_acts, mean, logstd)\n",
    "\n",
    "                policy_ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "                batch_gaes_expanded = batch_gaes.unsqueeze(1)\n",
    "                clipped_loss = clipped_ratio * batch_gaes_expanded\n",
    "                full_loss = policy_ratio * batch_gaes_expanded\n",
    "                policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "\n",
    "                probs = torch.exp(new_log_probs)\n",
    "                entropy = -torch.sum(probs * new_log_probs, dim=-1)\n",
    "                entropy_loss = torch.mean(entropy)\n",
    "\n",
    "                # Combine policy loss and entropy loss\n",
    "                total_loss = policy_loss - self.entropy_coeff * entropy_loss\n",
    "        \n",
    "                total_loss.backward()\n",
    "                self.policy_optim.step()\n",
    "\n",
    "  def train_value(self, obs, returns):\n",
    "        dataset_size = obs.size(0)\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        for _ in range(self.value_train_iters):\n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]    \n",
    "                batch_obs = obs[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                self.value_optim.zero_grad()\n",
    "                values = self.critic(batch_obs).squeeze()\n",
    "                \n",
    "                value_loss = (batch_returns - values) ** 2\n",
    "                value_loss = 0.5*value_loss.mean()\n",
    "        \n",
    "                value_loss.backward()\n",
    "                self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e75f5-5a43-42b6-b783-218ecb0b3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(policy_model):\n",
    "    env = gym.make(\"BipedalWalker-v3\")\n",
    "    test_episode_reward_list = []\n",
    "    test_step_list = []\n",
    "    max_steps = 1600\n",
    "    for _ in range(100):\n",
    "        epi_reward = 0\n",
    "        step = 0\n",
    "        obs = env.reset()[0]\n",
    "        while True:\n",
    "            with torch.inference_mode():\n",
    "                action, logstd = policy_model(torch.from_numpy(obs))\n",
    "                action = action.numpy()\n",
    "                next_state, reward, done, info, _ = env.step(action)\n",
    "                epi_reward += reward\n",
    "                step+=1\n",
    "                if done or step>=max_steps:\n",
    "                    break\n",
    "                else:\n",
    "                    obs = next_state\n",
    "        test_episode_reward_list.append(epi_reward)\n",
    "        test_step_list.append(step)    \n",
    "    print(f'Average Episode reward: {np.mean(test_episode_reward_list)} | Average step count: {np.mean(test_step_list)}')\n",
    "    env.close()\n",
    "    return np.mean(test_episode_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51748499-8735-44a5-9ce3-2c01701ff9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "action_size = env.action_space.shape[0]\n",
    "obs_size = env.observation_space.shape[0]\n",
    "policy_model = PolicyModel(obs_size, action_size)\n",
    "value_model = ValueModel(obs_size)\n",
    "gamma=0.99\n",
    "decay=0.95\n",
    "ppo = PPOTrainer(\n",
    "    policy_model,\n",
    "    value_model,\n",
    "    policy_lr = 0.0001,\n",
    "    value_lr = 0.001,\n",
    "    policy_train_iters = 8,\n",
    "    value_train_iters = 8,\n",
    "    batch_size=256)\n",
    "\n",
    "memory = [[], [], [], [], [], []] # obs, action, reward, value, act_log_prob, dones\n",
    "episode_reward_list = []\n",
    "max_episode_steps = 1600\n",
    "memory_size = 2048\n",
    "max_steps = 10240000\n",
    "step_count = 0\n",
    "episode_step = 0\n",
    "obs = env.reset()[0]\n",
    "episode_reward = 0\n",
    "\n",
    "while step_count<max_steps:\n",
    "    with torch.inference_mode():\n",
    "        mean, logstd = policy_model(torch.from_numpy(obs))\n",
    "        value = value_model(torch.from_numpy(obs))\n",
    "        mean = mean.numpy()\n",
    "        logstd = logstd.numpy()\n",
    "        value = value.squeeze().numpy()\n",
    "    \n",
    "        action = np.clip(mean + np.exp(logstd) * np.random.normal(loc=0.0, scale=1.0, size=logstd.shape), -1, 1)\n",
    "        act_log_prob = log_prob(action, mean, logstd)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "    episode_step += 1\n",
    "    episode_reward += reward\n",
    "    if episode_step >= max_episode_steps:\n",
    "        done = True\n",
    "        \n",
    "    for i, item in enumerate((obs, action, reward, value, act_log_prob, done)):\n",
    "            memory[i].append(item)\n",
    "        \n",
    "    step_count += 1\n",
    "    if done:\n",
    "        obs = env.reset()[0]\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        if len(episode_reward_list)%100 == 0:\n",
    "            avg_reward = np.mean(episode_reward_list[-100:])\n",
    "            print(f'Episode {len(episode_reward_list)} | Train Avg Reward {avg_reward:.1f} | Training Steps {step_count}')\n",
    "            test_avg_reward = test_model(policy_model)\n",
    "            if test_avg_reward >= 300:\n",
    "                print(f\"Training stopped early at episode {len(episode_reward_list)} due to reaching target average reward.\")\n",
    "                break\n",
    "    else:\n",
    "        obs = next_state\n",
    "    if step_count%memory_size == 0:\n",
    "        memory = [np.asarray(x) for x in memory] # obs, action, reward, value, act_log_prob, dones\n",
    "        memory[3] = compute_gaes(memory[2], memory[3], memory[5], gamma, decay, normalize=True)\n",
    "        memory[2] = discounted_rewards(memory[2], memory[5], gamma)\n",
    "\n",
    "         # Shuffle\n",
    "        permute_idxs = np.random.permutation(len(memory[0]))\n",
    "        \n",
    "        # Policy data\n",
    "        obs_ = torch.tensor(memory[0][permute_idxs], dtype=torch.float32)\n",
    "        acts_ = torch.tensor(memory[1][permute_idxs], dtype=torch.float32)\n",
    "        returns_ = torch.tensor(memory[2][permute_idxs], dtype=torch.float32)\n",
    "        gaes_ = torch.tensor(memory[3][permute_idxs], dtype=torch.float32)\n",
    "        act_log_probs_ = torch.tensor(memory[4][permute_idxs], dtype=torch.float32)\n",
    "        \n",
    "        # Train model\n",
    "        ppo.train_policy(obs_, acts_, act_log_probs_, gaes_)\n",
    "        ppo.train_value(obs_, returns_)\n",
    "        memory = [[], [], [], [], [], []]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0643cf-25f0-486a-b4e4-e9660d50b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode = 'human')\n",
    "epi_reward = 0\n",
    "step = 0\n",
    "obs = env.reset()[0]\n",
    "while True:\n",
    "    step+=1\n",
    "    with torch.inference_mode():\n",
    "        action, logstd = policy_model(torch.from_numpy(obs))\n",
    "        action = action.numpy()\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        epi_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            obs = next_state\n",
    "print(f'Episode reward: {epi_reward} | step count: {step}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02f327-3fb7-46f2-8889-7298f533f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_model.state_dict(), 'actor_model_weightsv1.2.pth')\n",
    "torch.save(value_model.state_dict(), 'critic_model_weightsv1.2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
